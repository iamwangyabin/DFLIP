#!/usr/bin/env python3
"""å°†DFLIP3K fakeç›®å½•ä¸­çš„æ‰€æœ‰JSONæ–‡ä»¶æ•´åˆæˆCSVå’Œ/æˆ–å¤§JSONæ–‡ä»¶ã€‚

ç”¨æ³•:
    # ç”ŸæˆCSVå’ŒJSON
    python tools/consolidate_json_files.py --fake-root /home/data/yabin/DFLIP3K/fake --output output
    
    # åªç”ŸæˆCSV
    python tools/consolidate_json_files.py --fake-root /home/data/yabin/DFLIP3K/fake --output output.csv --format csv
    
    # åªç”ŸæˆJSON
    python tools/consolidate_json_files.py --fake-root /home/data/yabin/DFLIP3K/fake --output output.json --format json
"""

import argparse
import json
import csv
from pathlib import Path
from typing import List, Dict, Any, Set
from tqdm import tqdm
from collections import defaultdict


def parse_args():
    parser = argparse.ArgumentParser(
        description="æ•´åˆæ‰€æœ‰JSONæ–‡ä»¶åˆ°CSVæˆ–å¤§JSONæ–‡ä»¶"
    )
    parser.add_argument(
        "--fake-root",
        type=str,
        required=True,
        help="Fakeå›¾ç‰‡çš„æ ¹ç›®å½•ï¼Œä¾‹å¦‚: /home/data/yabin/DFLIP3K/fake",
    )
    parser.add_argument(
        "--output",
        type=str,
        required=True,
        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆä¸å¸¦æ‰©å±•åä¼šè‡ªåŠ¨ç”ŸæˆCSVå’ŒJSONï¼Œå¸¦æ‰©å±•ååªç”Ÿæˆè¯¥æ ¼å¼ï¼‰",
    )
    parser.add_argument(
        "--format",
        type=str,
        choices=["csv", "json", "both"],
        default="both",
        help="è¾“å‡ºæ ¼å¼: csv, json, æˆ– both (é»˜è®¤: both)",
    )
    parser.add_argument(
        "--max-sample",
        type=int,
        default=None,
        help="æœ€å¤§é‡‡æ ·æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼Œé»˜è®¤å¤„ç†æ‰€æœ‰æ–‡ä»¶ï¼‰",
    )
    return parser.parse_args()


def iter_json_files(root: Path, max_sample: int = None):
    """ç”Ÿæˆå™¨ï¼šè¾¹éå†è¾¹yield JSONæ–‡ä»¶ï¼Œé¿å…é‡å¤IO"""
    if not root.exists():
        print(f"[é”™è¯¯] ç›®å½•ä¸å­˜åœ¨: {root}")
        return
    
    print("ğŸ“‚ å¼€å§‹æµå¼å¤„ç†JSONæ–‡ä»¶...")
    
    # å¦‚æœæœ‰max_sampleé™åˆ¶ï¼Œéœ€è¦å…ˆæ”¶é›†å†é‡‡æ ·
    if max_sample:
        all_files = list(root.rglob("*.json"))
        print(f"[é‡‡æ ·] ä»{len(all_files)}ä¸ªæ–‡ä»¶ä¸­é‡‡æ ·{max_sample}ä¸ª")
        import random
        random.seed(42)
        sampled = random.sample(all_files, min(max_sample, len(all_files)))
        for p in sampled:
            if p.is_file():
                yield p
    else:
        # æµå¼å¤„ç†ï¼Œè¾¹éå†è¾¹yield
        for p in root.rglob("*.json"):
            if p.is_file():
                yield p


def load_json_file(json_path: Path) -> Dict[str, Any]:
    """åŠ è½½å•ä¸ªJSONæ–‡ä»¶"""
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data
    except Exception as e:
        print(f"[è­¦å‘Š] æ— æ³•è¯»å– {json_path}: {e}")
        return {}


def analyze_json_structure(root: Path, sample_size: int = 100) -> Set[str]:
    """å¿«é€Ÿé‡‡æ ·åˆ†æJSONç»“æ„ï¼Œè·å–æ‰€æœ‰å¯èƒ½çš„å­—æ®µ"""
    all_keys = set()
    count = 0
    
    print(f"\nğŸ” å¿«é€Ÿé‡‡æ ·åˆ†æJSONç»“æ„ (æœ€å¤š{sample_size}ä¸ªæ–‡ä»¶)...")
    
    # ä½¿ç”¨ç”Ÿæˆå™¨ï¼Œé‡‡æ ·æŒ‡å®šæ•°é‡çš„æ–‡ä»¶
    pbar = tqdm(desc="ğŸ” é‡‡æ ·åˆ†æ", unit="ä¸ª", ncols=100, colour='blue')
    for json_path in root.rglob("*.json"):
        if count >= sample_size:
            break
        if json_path.is_file():
            data = load_json_file(json_path)
            if data:
                all_keys.update(flatten_dict(data).keys())
            count += 1
            pbar.update(1)
    pbar.close()
    
    print(f"   ä»{count}ä¸ªæ–‡ä»¶ä¸­å‘ç°å­—æ®µ")
    return all_keys


def flatten_dict(d: Dict[str, Any], parent_key: str = '', sep: str = '.') -> Dict[str, Any]:
    """æ‰å¹³åŒ–åµŒå¥—å­—å…¸"""
    items = []
    for k, v in d.items():
        new_key = f"{parent_key}{sep}{k}" if parent_key else k
        if isinstance(v, dict):
            items.extend(flatten_dict(v, new_key, sep=sep).items())
        elif isinstance(v, list):
            # å°†åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            items.append((new_key, json.dumps(v, ensure_ascii=False)))
        else:
            items.append((new_key, v))
    return dict(items)


def consolidate_to_json(fake_root: Path, output_path: Path, max_sample: int = None):
    """æµå¼æ•´åˆæ‰€æœ‰JSONåˆ°ä¸€ä¸ªå¤§JSONæ–‡ä»¶"""
    print(f"\nğŸ“¦ æµå¼æ•´åˆJSONæ–‡ä»¶åˆ°å¤§JSON...")
    
    consolidated = []
    count = 0
    
    # ä½¿ç”¨ç”Ÿæˆå™¨æµå¼å¤„ç†
    pbar = tqdm(desc="ğŸ“¦ æ•´åˆJSON", unit="ä¸ª", ncols=100, colour='cyan')
    for json_path in iter_json_files(fake_root, max_sample):
        data = load_json_file(json_path)
        if data:
            # æ·»åŠ å…ƒæ•°æ®
            try:
                rel_path = json_path.relative_to(fake_root)
                data['_meta_json_path'] = str(rel_path)
                data['_meta_filename'] = json_path.name
                
                # æå–familyå’Œsubmodel
                if len(rel_path.parts) >= 2:
                    data['_meta_family'] = rel_path.parts[0]
                    data['_meta_submodel'] = rel_path.parts[1]
            except ValueError:
                data['_meta_json_path'] = str(json_path)
            
            consolidated.append(data)
            count += 1
            pbar.update(1)
    pbar.close()
    
    # ä¿å­˜ä¸ºJSON
    print(f"ğŸ’¾ ä¿å­˜åˆ° {output_path}...")
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_path.parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(consolidated, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… æˆåŠŸä¿å­˜ {count} æ¡è®°å½•åˆ° {output_path}")
    print(f"   æ–‡ä»¶å¤§å°: {output_path.stat().st_size / 1024 / 1024:.2f} MB")


def consolidate_to_csv(fake_root: Path, output_path: Path, max_sample: int = None):
    """æµå¼æ•´åˆæ‰€æœ‰JSONåˆ°CSVæ–‡ä»¶"""
    print(f"\nğŸ“Š æµå¼æ•´åˆJSONæ–‡ä»¶åˆ°CSV...")
    
    # ç¬¬ä¸€æ­¥ï¼šå¿«é€Ÿé‡‡æ ·åˆ†æå­—æ®µç»“æ„
    all_keys = analyze_json_structure(fake_root, sample_size=100)
    
    # æ·»åŠ å…ƒæ•°æ®å­—æ®µ
    meta_fields = ['_meta_json_path', '_meta_filename', '_meta_family', '_meta_submodel']
    all_keys.update(meta_fields)
    
    # æŒ‰å­—æ¯æ’åºå­—æ®µï¼Œä½†å…ƒæ•°æ®å­—æ®µæ”¾åœ¨å‰é¢
    sorted_keys = meta_fields + sorted([k for k in all_keys if k not in meta_fields])
    
    print(f"ğŸ“‹ å‘ç° {len(sorted_keys)} ä¸ªå­—æ®µ")
    
    # ç¬¬äºŒæ­¥ï¼šæµå¼å†™å…¥CSV
    print(f"ğŸ’¾ æµå¼å†™å…¥åˆ° {output_path}...")
    count = 0
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_path.parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, 'w', encoding='utf-8', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=sorted_keys, extrasaction='ignore')
        writer.writeheader()
        
        # ä½¿ç”¨ç”Ÿæˆå™¨æµå¼å¤„ç†
        pbar = tqdm(desc="ğŸ“Š å†™å…¥CSV", unit="ä¸ª", ncols=100, colour='magenta')
        for json_path in iter_json_files(fake_root, max_sample):
            data = load_json_file(json_path)
            if data:
                # æ‰å¹³åŒ–æ•°æ®
                flat_data = flatten_dict(data)
                
                # æ·»åŠ å…ƒæ•°æ®
                try:
                    rel_path = json_path.relative_to(fake_root)
                    flat_data['_meta_json_path'] = str(rel_path)
                    flat_data['_meta_filename'] = json_path.name
                    
                    if len(rel_path.parts) >= 2:
                        flat_data['_meta_family'] = rel_path.parts[0]
                        flat_data['_meta_submodel'] = rel_path.parts[1]
                except ValueError:
                    flat_data['_meta_json_path'] = str(json_path)
                
                writer.writerow(flat_data)
                count += 1
                pbar.update(1)
        pbar.close()
    
    print(f"âœ… æˆåŠŸä¿å­˜ {count} æ¡è®°å½•åˆ° {output_path}")
    print(f"   æ–‡ä»¶å¤§å°: {output_path.stat().st_size / 1024 / 1024:.2f} MB")


def main():
    args = parse_args()
    fake_root = Path(args.fake_root).resolve()
    
    if not fake_root.exists():
        print(f"[é”™è¯¯] fake_rootä¸å­˜åœ¨: {fake_root}")
        return
    
    print(f"ğŸ“‚ å¤„ç†ç›®å½•: {fake_root}\n")
    print("="*70)
    
    # ç¡®å®šè¾“å‡ºæ ¼å¼
    output_path = Path(args.output)
    
    if args.format == "both" or (args.format == "both" and not output_path.suffix):
        # ç”Ÿæˆä¸¤ç§æ ¼å¼ï¼ˆæµå¼å¤„ç†ï¼Œé¿å…é‡å¤IOï¼‰
        base_path = output_path.with_suffix('')
        csv_path = base_path.with_suffix('.csv')
        json_path = base_path.with_suffix('.json')
        
        consolidate_to_csv(fake_root, csv_path, args.max_sample)
        consolidate_to_json(fake_root, json_path, args.max_sample)
        
        print("\n" + "="*70)
        print("ğŸ‰ æ•´åˆå®Œæˆï¼")
        print("="*70)
        print(f"ğŸ“Š CSVæ–‡ä»¶:  {csv_path}")
        print(f"ğŸ“¦ JSONæ–‡ä»¶: {json_path}")
        
    elif args.format == "csv" or (output_path.suffix.lower() == '.csv'):
        # åªç”ŸæˆCSVï¼ˆæµå¼å¤„ç†ï¼‰
        csv_path = output_path.with_suffix('.csv')
        consolidate_to_csv(fake_root, csv_path, args.max_sample)
        
        print("\n" + "="*70)
        print("ğŸ‰ æ•´åˆå®Œæˆï¼")
        print("="*70)
        print(f"ğŸ“Š CSVæ–‡ä»¶: {csv_path}")
        
    elif args.format == "json" or (output_path.suffix.lower() == '.json'):
        # åªç”ŸæˆJSONï¼ˆæµå¼å¤„ç†ï¼‰
        json_path = output_path.with_suffix('.json')
        consolidate_to_json(fake_root, json_path, args.max_sample)
        
        print("\n" + "="*70)
        print("ğŸ‰ æ•´åˆå®Œæˆï¼")
        print("="*70)
        print(f"ğŸ“¦ JSONæ–‡ä»¶: {json_path}")
    
    # æä¾›ä½¿ç”¨å»ºè®®
    print("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
    print("  - CSVæ ¼å¼: é€‚åˆåœ¨Excel/Numbersä¸­æ‰“å¼€ï¼Œæ–¹ä¾¿æ•°æ®åˆ†æå’Œç­›é€‰")
    print("  - JSONæ ¼å¼: ä¿ç•™å®Œæ•´çš„åµŒå¥—ç»“æ„ï¼Œé€‚åˆç¨‹åºåŒ–å¤„ç†")
    print("  - CSVä¸­çš„åµŒå¥—æ•°æ®å·²è¢«æ‰å¹³åŒ–ï¼ˆç”¨.åˆ†éš”ï¼‰ï¼Œåˆ—è¡¨ä¼šè½¬ä¸ºJSONå­—ç¬¦ä¸²")


if __name__ == "__main__":
    main()