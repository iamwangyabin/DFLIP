#!/usr/bin/env python3
"""å°†DFLIP3K fakeç›®å½•ä¸­çš„æ‰€æœ‰JSONæ–‡ä»¶æ•´åˆæˆCSVå’Œ/æˆ–å¤§JSONæ–‡ä»¶ã€‚

ç”¨æ³•:
    # ç”ŸæˆCSVå’ŒJSON
    python tools/consolidate_json_files.py --fake-root /home/data/yabin/DFLIP3K/fake --output output
    
    # åªç”ŸæˆCSV
    python tools/consolidate_json_files.py --fake-root /home/data/yabin/DFLIP3K/fake --output output.csv --format csv
    
    # åªç”ŸæˆJSON
    python tools/consolidate_json_files.py --fake-root /home/data/yabin/DFLIP3K/fake --output output.json --format json
"""

import argparse
import json
import csv
from pathlib import Path
from typing import List, Dict, Any, Set
from tqdm import tqdm
from collections import defaultdict


def parse_args():
    parser = argparse.ArgumentParser(
        description="æ•´åˆæ‰€æœ‰JSONæ–‡ä»¶åˆ°CSVæˆ–å¤§JSONæ–‡ä»¶"
    )
    parser.add_argument(
        "--fake-root",
        type=str,
        required=True,
        help="Fakeå›¾ç‰‡çš„æ ¹ç›®å½•ï¼Œä¾‹å¦‚: /home/data/yabin/DFLIP3K/fake",
    )
    parser.add_argument(
        "--output",
        type=str,
        required=True,
        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆä¸å¸¦æ‰©å±•åä¼šè‡ªåŠ¨ç”ŸæˆCSVå’ŒJSONï¼Œå¸¦æ‰©å±•ååªç”Ÿæˆè¯¥æ ¼å¼ï¼‰",
    )
    parser.add_argument(
        "--format",
        type=str,
        choices=["csv", "json", "both"],
        default="both",
        help="è¾“å‡ºæ ¼å¼: csv, json, æˆ– both (é»˜è®¤: both)",
    )
    parser.add_argument(
        "--max-sample",
        type=int,
        default=None,
        help="æœ€å¤§é‡‡æ ·æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼Œé»˜è®¤å¤„ç†æ‰€æœ‰æ–‡ä»¶ï¼‰",
    )
    return parser.parse_args()


def collect_json_files(root: Path, max_sample: int = None) -> List[Path]:
    """é€’å½’æ”¶é›†æ‰€æœ‰JSONæ–‡ä»¶"""
    files: List[Path] = []
    if not root.exists():
        print(f"[é”™è¯¯] ç›®å½•ä¸å­˜åœ¨: {root}")
        return files
    
    print("ğŸ“‚ æ­£åœ¨æ‰«æJSONæ–‡ä»¶...")
    all_files = list(root.rglob("*.json"))
    
    if max_sample and len(all_files) > max_sample:
        print(f"[é‡‡æ ·] ä»{len(all_files)}ä¸ªæ–‡ä»¶ä¸­é‡‡æ ·{max_sample}ä¸ª")
        import random
        random.seed(42)
        all_files = random.sample(all_files, max_sample)
    
    for p in tqdm(all_files, desc="ğŸ“‚ æ‰«æJSONæ–‡ä»¶", unit="ä¸ª", ncols=100, colour='green'):
        if p.is_file():
            files.append(p)
    
    return files


def load_json_file(json_path: Path) -> Dict[str, Any]:
    """åŠ è½½å•ä¸ªJSONæ–‡ä»¶"""
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data
    except Exception as e:
        print(f"[è­¦å‘Š] æ— æ³•è¯»å– {json_path}: {e}")
        return {}


def analyze_json_structure(json_files: List[Path], sample_size: int = 100) -> Set[str]:
    """åˆ†æJSONç»“æ„ï¼Œè·å–æ‰€æœ‰å¯èƒ½çš„å­—æ®µ"""
    all_keys = set()
    sample_files = json_files[:sample_size] if len(json_files) > sample_size else json_files
    
    print(f"\nğŸ” åˆ†æJSONç»“æ„ (é‡‡æ ·{len(sample_files)}ä¸ªæ–‡ä»¶)...")
    for json_path in tqdm(sample_files, desc="ğŸ” åˆ†æç»“æ„", unit="ä¸ª", ncols=100, colour='blue'):
        data = load_json_file(json_path)
        if data:
            all_keys.update(flatten_dict(data).keys())
    
    return all_keys


def flatten_dict(d: Dict[str, Any], parent_key: str = '', sep: str = '.') -> Dict[str, Any]:
    """æ‰å¹³åŒ–åµŒå¥—å­—å…¸"""
    items = []
    for k, v in d.items():
        new_key = f"{parent_key}{sep}{k}" if parent_key else k
        if isinstance(v, dict):
            items.extend(flatten_dict(v, new_key, sep=sep).items())
        elif isinstance(v, list):
            # å°†åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            items.append((new_key, json.dumps(v, ensure_ascii=False)))
        else:
            items.append((new_key, v))
    return dict(items)


def consolidate_to_json(json_files: List[Path], fake_root: Path, output_path: Path):
    """æ•´åˆæ‰€æœ‰JSONåˆ°ä¸€ä¸ªå¤§JSONæ–‡ä»¶"""
    print(f"\nğŸ“¦ æ­£åœ¨æ•´åˆ {len(json_files)} ä¸ªJSONæ–‡ä»¶åˆ°å¤§JSON...")
    
    consolidated = []
    
    for json_path in tqdm(json_files, desc="ğŸ“¦ æ•´åˆJSON", unit="ä¸ª", ncols=100, colour='cyan'):
        data = load_json_file(json_path)
        if data:
            # æ·»åŠ å…ƒæ•°æ®
            try:
                rel_path = json_path.relative_to(fake_root)
                data['_meta_json_path'] = str(rel_path)
                data['_meta_filename'] = json_path.name
                
                # æå–familyå’Œsubmodel
                if len(rel_path.parts) >= 2:
                    data['_meta_family'] = rel_path.parts[0]
                    data['_meta_submodel'] = rel_path.parts[1]
            except ValueError:
                data['_meta_json_path'] = str(json_path)
            
            consolidated.append(data)
    
    # ä¿å­˜ä¸ºJSON
    print(f"ğŸ’¾ ä¿å­˜åˆ° {output_path}...")
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(consolidated, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… æˆåŠŸä¿å­˜ {len(consolidated)} æ¡è®°å½•åˆ° {output_path}")
    print(f"   æ–‡ä»¶å¤§å°: {output_path.stat().st_size / 1024 / 1024:.2f} MB")


def consolidate_to_csv(json_files: List[Path], fake_root: Path, output_path: Path):
    """æ•´åˆæ‰€æœ‰JSONåˆ°CSVæ–‡ä»¶"""
    print(f"\nğŸ“Š æ­£åœ¨æ•´åˆ {len(json_files)} ä¸ªJSONæ–‡ä»¶åˆ°CSV...")
    
    # é¦–å…ˆåˆ†ææ‰€æœ‰å­—æ®µ
    all_keys = analyze_json_structure(json_files)
    
    # æ·»åŠ å…ƒæ•°æ®å­—æ®µ
    meta_fields = ['_meta_json_path', '_meta_filename', '_meta_family', '_meta_submodel']
    all_keys.update(meta_fields)
    
    # æŒ‰å­—æ¯æ’åºå­—æ®µï¼Œä½†å…ƒæ•°æ®å­—æ®µæ”¾åœ¨å‰é¢
    sorted_keys = meta_fields + sorted([k for k in all_keys if k not in meta_fields])
    
    print(f"ğŸ“‹ å‘ç° {len(sorted_keys)} ä¸ªå­—æ®µ")
    
    # å†™å…¥CSV
    print(f"ğŸ’¾ ä¿å­˜åˆ° {output_path}...")
    with open(output_path, 'w', encoding='utf-8', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=sorted_keys, extrasaction='ignore')
        writer.writeheader()
        
        for json_path in tqdm(json_files, desc="ğŸ“Š å†™å…¥CSV", unit="ä¸ª", ncols=100, colour='magenta'):
            data = load_json_file(json_path)
            if data:
                # æ‰å¹³åŒ–æ•°æ®
                flat_data = flatten_dict(data)
                
                # æ·»åŠ å…ƒæ•°æ®
                try:
                    rel_path = json_path.relative_to(fake_root)
                    flat_data['_meta_json_path'] = str(rel_path)
                    flat_data['_meta_filename'] = json_path.name
                    
                    if len(rel_path.parts) >= 2:
                        flat_data['_meta_family'] = rel_path.parts[0]
                        flat_data['_meta_submodel'] = rel_path.parts[1]
                except ValueError:
                    flat_data['_meta_json_path'] = str(json_path)
                
                writer.writerow(flat_data)
    
    print(f"âœ… æˆåŠŸä¿å­˜ {len(json_files)} æ¡è®°å½•åˆ° {output_path}")
    print(f"   æ–‡ä»¶å¤§å°: {output_path.stat().st_size / 1024 / 1024:.2f} MB")


def main():
    args = parse_args()
    fake_root = Path(args.fake_root).resolve()
    
    if not fake_root.exists():
        print(f"[é”™è¯¯] fake_rootä¸å­˜åœ¨: {fake_root}")
        return
    
    print(f"ğŸ“‚ å¤„ç†ç›®å½•: {fake_root}\n")
    print("="*70)
    
    # æ”¶é›†JSONæ–‡ä»¶
    json_files = collect_json_files(fake_root, args.max_sample)
    
    if not json_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•JSONæ–‡ä»¶")
        return
    
    print(f"\nâœ… æ‰¾åˆ° {len(json_files)} ä¸ªJSONæ–‡ä»¶")
    print("="*70)
    
    # ç¡®å®šè¾“å‡ºæ ¼å¼
    output_path = Path(args.output)
    
    if args.format == "both" or (args.format == "both" and not output_path.suffix):
        # ç”Ÿæˆä¸¤ç§æ ¼å¼
        base_path = output_path.with_suffix('')
        csv_path = base_path.with_suffix('.csv')
        json_path = base_path.with_suffix('.json')
        
        consolidate_to_csv(json_files, fake_root, csv_path)
        consolidate_to_json(json_files, fake_root, json_path)
        
        print("\n" + "="*70)
        print("ğŸ‰ æ•´åˆå®Œæˆï¼")
        print("="*70)
        print(f"ğŸ“Š CSVæ–‡ä»¶:  {csv_path}")
        print(f"ğŸ“¦ JSONæ–‡ä»¶: {json_path}")
        
    elif args.format == "csv" or (output_path.suffix.lower() == '.csv'):
        # åªç”ŸæˆCSV
        csv_path = output_path.with_suffix('.csv')
        consolidate_to_csv(json_files, fake_root, csv_path)
        
        print("\n" + "="*70)
        print("ğŸ‰ æ•´åˆå®Œæˆï¼")
        print("="*70)
        print(f"ğŸ“Š CSVæ–‡ä»¶: {csv_path}")
        
    elif args.format == "json" or (output_path.suffix.lower() == '.json'):
        # åªç”ŸæˆJSON
        json_path = output_path.with_suffix('.json')
        consolidate_to_json(json_files, fake_root, json_path)
        
        print("\n" + "="*70)
        print("ğŸ‰ æ•´åˆå®Œæˆï¼")
        print("="*70)
        print(f"ğŸ“¦ JSONæ–‡ä»¶: {json_path}")
    
    # æä¾›ä½¿ç”¨å»ºè®®
    print("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
    print("  - CSVæ ¼å¼: é€‚åˆåœ¨Excel/Numbersä¸­æ‰“å¼€ï¼Œæ–¹ä¾¿æ•°æ®åˆ†æå’Œç­›é€‰")
    print("  - JSONæ ¼å¼: ä¿ç•™å®Œæ•´çš„åµŒå¥—ç»“æ„ï¼Œé€‚åˆç¨‹åºåŒ–å¤„ç†")
    print("  - CSVä¸­çš„åµŒå¥—æ•°æ®å·²è¢«æ‰å¹³åŒ–ï¼ˆç”¨.åˆ†éš”ï¼‰ï¼Œåˆ—è¡¨ä¼šè½¬ä¸ºJSONå­—ç¬¦ä¸²")


if __name__ == "__main__":
    main()