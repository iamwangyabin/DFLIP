#!/usr/bin/env python3
"""Process images from dflip3k_meta.json: convert to JPEG and resize if needed.

This script reads the metadata JSON generated by generate_dataset_meta_dflip3k.py
and processes images with the following transformations:
1. Convert all images to JPEG format with split-specific quality (train=100, test=80)
2. For TEST images only: If minimum dimension > 512, resize so minimum dimension = 512 (preserving aspect ratio)
3. For TRAIN images: Keep original dimensions (no resizing)
4. Use different interpolation algorithms for resizing (LANCZOS, BICUBIC, BILINEAR)
5. Organize output into train/ and test/ subdirectories based on split

The script uses multiprocessing for efficient parallel processing.

Example usage:
    python tools/process_images_to_jpeg.py \
        --meta-json ./assets/dflip3k_meta.json \
        --image-root /home/data/yabin/DFLIP3K \
        --output-root /home/data/yabin/DFLIP3K_processed \
        --workers 8
"""

import argparse
import json
import multiprocessing as mp
from pathlib import Path
from typing import Dict, List, Optional
import sys
import random

try:
    from PIL import Image
except ImportError:
    print("Error: PIL (Pillow) is required. Install with: pip install Pillow")
    sys.exit(1)

from tqdm import tqdm


# Supported interpolation methods
RESIZE_METHODS = {
    'LANCZOS': Image.LANCZOS,
    'BICUBIC': Image.BICUBIC,
    'BILINEAR': Image.BILINEAR,
    'NEAREST': Image.NEAREST,
}


def parse_args():
    parser = argparse.ArgumentParser(
        description="Process images: convert to JPEG and resize test images only"
    )
    parser.add_argument(
        "--meta-json",
        type=str,
        required=True,
        help="Path to metadata JSON file (output from generate_dataset_meta_dflip3k.py)",
    )
    parser.add_argument(
        "--image-root",
        type=str,
        required=True,
        help="Root directory of original images (same as used in generate_dataset_meta_dflip3k.py)",
    )
    parser.add_argument(
        "--output-root",
        type=str,
        required=True,
        help="Root directory for processed images",
    )
    parser.add_argument(
        "--workers",
        type=int,
        default=4,
        help="Number of worker processes (default: 4)",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=None,
        help="Random seed for reproducible resize method selection (default: None for random)",
    )
    parser.add_argument(
        "--min-size",
        type=int,
        default=512,
        help="Target minimum dimension for resizing TEST images only (default: 512). Train images keep original size.",
    )
    parser.add_argument(
        "--jpeg-quality",
        type=int,
        default=90,
        help="JPEG quality for unknown splits (1-100, default: 90). Train=100, Test=80 automatically.",
    )
    parser.add_argument(
        "--preserve-structure",
        action="store_true",
        help="Preserve the original directory structure in output",
    )
    parser.add_argument(
        "--skip-existing",
        action="store_true",
        help="Skip processing if output file already exists",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Print processing plan without actually processing images",
    )
    parser.add_argument(
        "--output-meta",
        type=str,
        default=None,
        help="Path to save updated metadata JSON with processed image paths (default: same as input with '_processed' suffix)",
    )
    return parser.parse_args()


def process_single_image(args_tuple):
    """Process a single image: convert to JPEG and resize if needed.
    
    Args:
        args_tuple: Tuple of (record, image_root, output_root, config, index)
        
    Returns:
        Tuple of (success: bool, src_path: str, dst_path: str, message: str)
    """
    record, image_root, output_root, config, index = args_tuple
    
    # Randomly select a resize method for this image
    # Use index for reproducibility if seed is set
    if config.get('seed') is not None:
        random.seed(config['seed'] + index)
    
    resize_method_name = random.choice(list(RESIZE_METHODS.keys()))
    resize_method = RESIZE_METHODS[resize_method_name]
    
    min_size = config['min_size']
    preserve_structure = config['preserve_structure']
    skip_existing = config['skip_existing']
    
    # Set JPEG quality based on split: train=100, test=80
    split = record.get('split', 'train')  # Default to train if split not found
    if split == 'train':
        jpeg_quality = 100
    elif split == 'test':
        jpeg_quality = 80
    else:
        jpeg_quality = config.get('jpeg_quality', 90)  # Fallback for unknown splits
    
    src_path = Path(image_root) / record['image_path']
    
    # Determine output path - always organize by split (train/test)
    if preserve_structure:
        # Keep the same relative path structure under split directory
        rel_path = Path(record['image_path'])
        # Change extension to .jpg
        rel_path = rel_path.with_suffix('.jpg')
        dst_path = Path(output_root) / split / rel_path
    else:
        # Flatten structure: use image filename only under split directory
        filename = Path(record['image_path']).stem + '.jpg'
        dst_path = Path(output_root) / split / filename
    
    # Check if source exists
    if not src_path.exists():
        return False, str(src_path), str(dst_path), f"Source not found"
    
    # Skip if output exists and skip_existing is True
    if skip_existing and dst_path.exists():
        return True, str(src_path), str(dst_path), "Skipped (already exists)"
    
    try:
        # Create output directory
        dst_path.parent.mkdir(parents=True, exist_ok=True)
        
        # Open and convert image
        with Image.open(src_path) as img:
            # Convert to RGB if needed (for PNG with transparency, etc.)
            if img.mode not in ('RGB', 'L'):
                img = img.convert('RGB')
            elif img.mode == 'L':  # Grayscale
                img = img.convert('RGB')
            
            # Get current size
            width, height = img.size
            min_dim = min(width, height)
            
            # Only resize test images, keep training images at original size
            if split == 'test' and min_dim > min_size:
                # Calculate new size maintaining aspect ratio
                if width < height:
                    new_width = min_size
                    new_height = int(height * (min_size / width))
                else:
                    new_height = min_size
                    new_width = int(width * (min_size / height))
                
                img = img.resize(
                    (new_width, new_height),
                    resample=resize_method
                )
                resize_info = f"Resized from {width}x{height} to {new_width}x{new_height} ({resize_method_name})"
            else:
                if split == 'train':
                    resize_info = f"Train image - no resize ({width}x{height})"
                else:
                    resize_info = f"No resize needed ({width}x{height})"
            
            # Save as JPEG with split-specific quality
            img.save(
                dst_path,
                'JPEG',
                quality=jpeg_quality,
                optimize=True
            )
        
        return True, str(src_path), str(dst_path), f"{resize_info}, Quality: {jpeg_quality} ({split})"
        
    except Exception as e:
        return False, str(src_path), str(dst_path), f"Error: {str(e)}"


def load_metadata(meta_json_path: str) -> List[Dict]:
    """Load metadata from JSON file."""
    with open(meta_json_path, 'r', encoding='utf-8') as f:
        records = json.load(f)
    return records


def print_statistics(records: List[Dict]):
    """Print statistics about the dataset."""
    total = len(records)
    fake = sum(1 for r in records if r['is_fake'] == 1)
    real = sum(1 for r in records if r['is_fake'] == 0)
    train = sum(1 for r in records if r['split'] == 'train')
    test = sum(1 for r in records if r['split'] == 'test')
    
    print(f"\n{'='*60}")
    print(f"Dataset Statistics:")
    print(f"{'='*60}")
    print(f"Total images:  {total}")
    print(f"  Fake:        {fake} ({100*fake/total:.1f}%)")
    print(f"  Real:        {real} ({100*real/total:.1f}%)")
    print(f"  Train split: {train} ({100*train/total:.1f}%)")
    print(f"  Test split:  {test} ({100*test/total:.1f}%)")
    print(f"{'='*60}\n")


def main():
    args = parse_args()
    
    # Load metadata
    print(f"Loading metadata from {args.meta_json}")
    records = load_metadata(args.meta_json)
    print(f"Loaded {len(records)} image records")
    
    # Print statistics
    print_statistics(records)
    
    # Prepare configuration
    config = {
        'min_size': args.min_size,
        'jpeg_quality': args.jpeg_quality,
        'preserve_structure': args.preserve_structure,
        'skip_existing': args.skip_existing,
        'seed': args.seed,
    }
    
    print(f"\nProcessing Configuration:")
    print(f"  Image root:       {args.image_root}")
    print(f"  Output root:      {args.output_root}")
    print(f"  Workers:          {args.workers}")
    print(f"  Resize methods:   Random from {list(RESIZE_METHODS.keys())} (test images only)")
    print(f"  Random seed:      {args.seed if args.seed is not None else 'None (random)'}")
    print(f"  Min size:         {args.min_size} (test images only, train images keep original size)")
    print(f"  JPEG quality:     Train=100, Test=80, Unknown={args.jpeg_quality}")
    print(f"  Preserve struct:  {args.preserve_structure}")
    print(f"  Skip existing:    {args.skip_existing}")
    print(f"  Output structure: {args.output_root}/{{train|test}}/...")
    
    if args.dry_run:
        print("\n[DRY RUN] Would process the following images:")
        for i, record in enumerate(records[:10]):  # Show first 10
            src = Path(args.image_root) / record['image_path']
            print(f"  {i+1}. {src}")
        if len(records) > 10:
            print(f"  ... and {len(records) - 10} more")
        return
    
    # Create output directory
    Path(args.output_root).mkdir(parents=True, exist_ok=True)
    
    # Prepare arguments for workers (include index for reproducible randomness)
    worker_args = [
        (record, args.image_root, args.output_root, config, idx)
        for idx, record in enumerate(records)
    ]
    
    # Process images with multiprocessing
    print(f"\nProcessing {len(records)} images with {args.workers} workers...")
    
    success_count = 0
    error_count = 0
    errors = []
    
    with mp.Pool(processes=args.workers) as pool:
        results = list(tqdm(
            pool.imap(process_single_image, worker_args),
            total=len(records),
            desc="Processing images",
            unit="img"
        ))
    
    # Collect results
    for success, src, dst, msg in results:
        if success:
            success_count += 1
        else:
            error_count += 1
            errors.append((src, msg))
    
    # Print summary
    print(f"\n{'='*60}")
    print(f"Processing Complete:")
    print(f"{'='*60}")
    print(f"Successfully processed: {success_count}/{len(records)}")
    print(f"Errors:                {error_count}/{len(records)}")
    
    if errors:
        print(f"\nErrors encountered:")
        for src, msg in errors[:20]:  # Show first 20 errors
            print(f"  {src}: {msg}")
        if len(errors) > 20:
            print(f"  ... and {len(errors) - 20} more errors")
    
    print(f"\nOutput saved to: {args.output_root}")
    print(f"{'='*60}")
    
    # Update metadata JSON with new paths if requested
    if success_count > 0:
        print(f"\nNote: Images organized by split into train/ and test/ directories.")
        if args.preserve_structure:
            print(f"Original directory structure preserved within each split.")
        print(f"Train images (quality=100): {args.output_root}/train/")
        print(f"Test images (quality=80):   {args.output_root}/test/")


if __name__ == "__main__":
    main()