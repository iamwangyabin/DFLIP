#!/usr/bin/env python3
"""Convert consolidated JSON metadata to LLaVA format for multimodal training.

This script processes the consolidated JSON file generated by consolidate_json_files.py
and converts records with valid prompts into LLaVA conversation format. It validates
that corresponding image files actually exist by scanning the specified image directory.

Usage:
    python tools/convert_to_llava_format.py \
        --input consolidated.json \
        --output llava_dataset.json \
        --image-dir /media/DATASET/person_data/dflip3k/fake \
        --image-ext .jpg

Features:
    - Recursively scans image directory to build ID-to-path mapping
    - Only includes records for images that actually exist
    - Uses real relative paths instead of generated filenames
    - Provides detailed statistics on image availability
"""

import argparse
import json
import re
from pathlib import Path
from typing import List, Dict, Any, Optional
from tqdm import tqdm
from collections import defaultdict


def parse_args():
    parser = argparse.ArgumentParser(
        description="Convert consolidated JSON to LLaVA format"
    )
    parser.add_argument(
        "--input",
        type=str,
        required=True,
        help="Input consolidated JSON file path"
    )
    parser.add_argument(
        "--output", 
        type=str,
        required=True,
        help="Output LLaVA format JSON file path"
    )
    parser.add_argument(
        "--image-ext",
        type=str,
        default=".jpg",
        help="Image file extension (default: .jpg)"
    )
    parser.add_argument(
        "--question-template",
        type=str,
        default="<image>\nDescribe this image in detail.",
        help="Question template for human input"
    )
    parser.add_argument(
        "--min-prompt-length",
        type=int,
        default=10,
        help="Minimum prompt length to include (default: 10)"
    )
    parser.add_argument(
        "--max-prompt-length",
        type=int,
        default=2000,
        help="Maximum prompt length to include (default: 2000)"
    )
    parser.add_argument(
        "--image-dir",
        type=str,
        default="/media/DATASET/person_data/dflip3k/fake",
        help="Directory containing actual image files (default: /media/DATASET/person_data/dflip3k/fake)"
    )
    parser.add_argument(
        "--max-records",
        type=int,
        default=None,
        help="Maximum number of records to process (for testing)"
    )
    parser.add_argument(
        "--stats-only",
        action="store_true",
        help="Only show statistics without generating output file"
    )
    return parser.parse_args()


def clean_prompt(prompt: str) -> str:
    """Clean and normalize prompt text."""
    if not prompt:
        return ""
    
    # Remove excessive whitespace
    prompt = re.sub(r'\s+', ' ', prompt.strip())
    
    # Remove common unwanted patterns
    unwanted_patterns = [
        r'\bfully clothed no boobs\b',
        r'\bno nudity\b',
        r'\bsfw\b',
        r'\bnsfw\b',
    ]
    
    for pattern in unwanted_patterns:
        prompt = re.sub(pattern, '', prompt, flags=re.IGNORECASE)
    
    # Clean up extra spaces after removal
    prompt = re.sub(r'\s+', ' ', prompt.strip())
    
    return prompt


def is_valid_prompt(prompt: str, min_length: int, max_length: int) -> bool:
    """Check if prompt is valid for training."""
    if not prompt or not isinstance(prompt, str):
        return False
    
    cleaned = clean_prompt(prompt)
    
    # Check length constraints
    if len(cleaned) < min_length or len(cleaned) > max_length:
        return False
    
    # Check for meaningful content (not just punctuation/numbers)
    if not re.search(r'[a-zA-Z]', cleaned):
        return False
    
    return True


def extract_prompt_from_record(record: Dict[str, Any]) -> Optional[str]:
    """Extract prompt from a record using comprehensive 3-tier strategy."""
    
    # Tier 1: Standard WebUI format - check root level prompt
    if record.get('prompt'):
        return str(record['prompt'])
    
    # Tier 2: Civitai metadata format - check meta.prompt
    if record.get('meta') and isinstance(record['meta'], dict):
        prompt = record['meta'].get('prompt')
        if prompt:
            return str(prompt)
    
    # Tier 3: ComfyUI workflow format - parse comfy JSON and extract from nodes
    if (record.get('meta') and isinstance(record['meta'], dict) and
        record['meta'].get('comfy')):
        
        comfy_prompt = extract_comfyui_prompt(record['meta']['comfy'])
        if comfy_prompt:
            return comfy_prompt
    
    # Fallback: Try other possible locations
    fallback_fields = ['description', 'caption']
    for field in fallback_fields:
        if record.get(field):
            return str(record[field])
    
    return None


def extract_comfyui_prompt(comfy_data: str) -> Optional[str]:
    """Extract prompt from ComfyUI workflow JSON string."""
    try:
        # Parse the ComfyUI JSON string
        if isinstance(comfy_data, str):
            comfy_json = json.loads(comfy_data)
        else:
            comfy_json = comfy_data
        
        # Look for prompt in the workflow structure
        if isinstance(comfy_json, dict) and 'prompt' in comfy_json:
            prompt_nodes = comfy_json['prompt']
            
            # Iterate through all nodes to find text inputs
            for node_id, node_data in prompt_nodes.items():
                if isinstance(node_data, dict) and 'inputs' in node_data:
                    inputs = node_data['inputs']
                    
                    # Check for standard text input (CLIPTextEncode)
                    if 'text' in inputs and inputs['text']:
                        text = inputs['text']
                        if isinstance(text, str) and len(text.strip()) > 0:
                            return text.strip()
                    
                    # Check for positive text input (MilehighStyler)
                    if 'text_positive' in inputs and inputs['text_positive']:
                        text = inputs['text_positive']
                        if isinstance(text, str) and len(text.strip()) > 0:
                            return text.strip()
                    
                    # Check for text2 input (ShowText|pysssss)
                    if 'text2' in inputs and inputs['text2']:
                        text = inputs['text2']
                        if isinstance(text, str) and len(text.strip()) > 0:
                            return text.strip()
        
        return None
        
    except (json.JSONDecodeError, TypeError, KeyError, AttributeError) as e:
        # If JSON parsing fails or structure is unexpected, return None
        return None


def build_image_mapping(image_dir: str) -> Dict[str, str]:
    """Build mapping from image ID to relative path by scanning image directory.
    
    Args:
        image_dir: Directory path to scan for images
        
    Returns:
        Dictionary mapping image_id (filename without extension) to relative path
    """
    image_mapping = {}
    image_dir_path = Path(image_dir)
    
    if not image_dir_path.exists():
        print(f"âš ï¸  Warning: Image directory does not exist: {image_dir}")
        return image_mapping
    
    print(f"ğŸ” Scanning image directory: {image_dir}")
    
    # Recursively find all .jpg files
    jpg_files = list(image_dir_path.rglob("*.jpg"))
    
    print(f"ğŸ“¸ Found {len(jpg_files):,} .jpg files")
    
    for jpg_file in tqdm(jpg_files, desc="Building image mapping", unit="files", ncols=100):
        # Get relative path from the image directory
        relative_path = jpg_file.relative_to(image_dir_path)
        
        # Extract ID (filename without extension)
        image_id = jpg_file.stem
        
        # Store mapping
        image_mapping[image_id] = str(relative_path)
    
    print(f"âœ… Built mapping for {len(image_mapping):,} unique image IDs")
    return image_mapping


def convert_record_to_llava(record: Dict[str, Any], question_template: str,
                           image_ext: str, min_length: int, max_length: int,
                           image_mapping: Dict[str, str]) -> Optional[Dict[str, Any]]:
    """Convert a single record to LLaVA format, only if corresponding image exists."""
    # Extract ID from _meta_filename or _meta_json_path
    record_id = None
    
    # First try to get ID from _meta_filename
    if record.get('_meta_filename'):
        filename = record['_meta_filename']
        if filename.endswith('.json'):
            record_id = filename[:-5]  # Remove .json extension
    
    # If not found, try to extract from _meta_json_path
    if not record_id and record.get('_meta_json_path'):
        json_path = record['_meta_json_path']
        # Extract filename from path and remove .json extension
        filename = Path(json_path).name
        if filename.endswith('.json'):
            record_id = filename[:-5]  # Remove .json extension
    
    # Fallback to original 'id' field if neither meta field is available
    if not record_id:
        record_id = record.get('id')
    
    if not record_id:
        return None
    
    # Check if image actually exists in the mapping
    if record_id not in image_mapping:
        return None  # Skip records without corresponding images
    
    # Extract prompt
    raw_prompt = extract_prompt_from_record(record)
    if not raw_prompt:
        return None
    
    # Clean and validate prompt
    cleaned_prompt = clean_prompt(raw_prompt)
    if not is_valid_prompt(cleaned_prompt, min_length, max_length):
        return None
    
    # Use actual image path from mapping
    image_path = image_mapping[record_id]
    
    # Create LLaVA format record
    llava_record = {
        "id": str(record_id),
        "image": image_path,
        "conversations": [
            {
                "from": "human",
                "value": question_template
            },
            {
                "from": "gpt",
                "value": cleaned_prompt
            }
        ]
    }
    
    return llava_record


def analyze_dataset_stats(records: List[Dict[str, Any]], image_mapping: Dict[str, str] = None) -> Dict[str, Any]:
    """Analyze dataset statistics."""
    stats = {
        'total_records': len(records),
        'records_with_meta': 0,
        'records_with_prompt': 0,
        'valid_prompts': 0,
        'records_with_images': 0,
        'records_missing_images': 0,
        'total_images_available': len(image_mapping) if image_mapping else 0,
        'prompt_lengths': [],
        'model_families': defaultdict(int),
        'base_models': defaultdict(int)
    }
    
    for record in records:
        # Count records with meta
        if record.get('meta'):
            stats['records_with_meta'] += 1
        
        # Extract record ID for image checking
        record_id = None
        if record.get('_meta_filename'):
            filename = record['_meta_filename']
            if filename.endswith('.json'):
                record_id = filename[:-5]
        elif record.get('_meta_json_path'):
            json_path = record['_meta_json_path']
            filename = Path(json_path).name
            if filename.endswith('.json'):
                record_id = filename[:-5]
        else:
            record_id = record.get('id')
        
        # Check if image exists
        if image_mapping and record_id:
            if record_id in image_mapping:
                stats['records_with_images'] += 1
            else:
                stats['records_missing_images'] += 1
        
        # Count records with prompts
        prompt = extract_prompt_from_record(record)
        if prompt:
            stats['records_with_prompt'] += 1
            cleaned = clean_prompt(prompt)
            if cleaned:
                stats['prompt_lengths'].append(len(cleaned))
                if is_valid_prompt(cleaned, 10, 2000):
                    stats['valid_prompts'] += 1
        
        # Count model families
        if record.get('_meta_family'):
            stats['model_families'][record['_meta_family']] += 1
        
        # Count base models
        if record.get('baseModel'):
            stats['base_models'][record['baseModel']] += 1
    
    # Calculate prompt length statistics
    if stats['prompt_lengths']:
        stats['avg_prompt_length'] = sum(stats['prompt_lengths']) / len(stats['prompt_lengths'])
        stats['min_prompt_length'] = min(stats['prompt_lengths'])
        stats['max_prompt_length'] = max(stats['prompt_lengths'])
    else:
        stats['avg_prompt_length'] = 0
        stats['min_prompt_length'] = 0
        stats['max_prompt_length'] = 0
    
    return stats


def print_statistics(stats: Dict[str, Any]):
    """Print dataset statistics in a formatted way."""
    print("\n" + "="*70)
    print("ğŸ“Š DATASET STATISTICS")
    print("="*70)
    
    print(f"ğŸ“ Total records: {stats['total_records']:,}")
    print(f"ğŸ”§ Records with metadata: {stats['records_with_meta']:,}")
    print(f"ğŸ’¬ Records with prompts: {stats['records_with_prompt']:,}")
    print(f"âœ… Valid prompts for training: {stats['valid_prompts']:,}")
    
    # Image availability statistics
    if stats.get('total_images_available', 0) > 0:
        print(f"\nğŸ“¸ Image Availability:")
        print(f"   Total images found: {stats['total_images_available']:,}")
        print(f"   Records with images: {stats['records_with_images']:,}")
        print(f"   Records missing images: {stats['records_missing_images']:,}")
        
        if stats['records_with_images'] > 0:
            image_match_rate = (stats['records_with_images'] / stats['total_records']) * 100
            print(f"   Image match rate: {image_match_rate:.2f}%")
    
    if stats['valid_prompts'] > 0:
        conversion_rate = (stats['valid_prompts'] / stats['total_records']) * 100
        print(f"ğŸ“ˆ Overall conversion rate: {conversion_rate:.2f}%")
    
    print(f"\nğŸ“ Prompt Length Statistics:")
    print(f"   Average: {stats['avg_prompt_length']:.1f} characters")
    print(f"   Range: {stats['min_prompt_length']} - {stats['max_prompt_length']} characters")
    
    print(f"\nğŸ·ï¸  Top Model Families:")
    for family, count in sorted(stats['model_families'].items(),
                               key=lambda x: x[1], reverse=True)[:5]:
        print(f"   {family}: {count:,} records")
    
    print(f"\nğŸ¤– Top Base Models:")
    for model, count in sorted(stats['base_models'].items(),
                              key=lambda x: x[1], reverse=True)[:5]:
        print(f"   {model}: {count:,} records")


def main():
    args = parse_args()
    
    input_path = Path(args.input)
    if not input_path.exists():
        print(f"âŒ Input file not found: {input_path}")
        return
    
    print(f"ğŸ“‚ Loading consolidated JSON from: {input_path}")
    
    # Load input data
    try:
        with open(input_path, 'r', encoding='utf-8') as f:
            records = json.load(f)
    except Exception as e:
        print(f"âŒ Error loading input file: {e}")
        return
    
    if not isinstance(records, list):
        print("âŒ Input file should contain a JSON array")
        return
    
    print(f"ğŸ“Š Loaded {len(records):,} records")
    
    # Build image mapping from the specified directory
    print(f"\nğŸ–¼ï¸  Building image mapping...")
    image_mapping = build_image_mapping(args.image_dir)
    
    if not image_mapping:
        print("âš ï¸  Warning: No images found in the specified directory!")
        print("   The output will be empty unless images are available.")
    
    # Limit records if specified
    if args.max_records:
        records = records[:args.max_records]
        print(f"ğŸ”¢ Limited to {len(records):,} records for processing")
    
    # Analyze statistics
    print("\nğŸ” Analyzing dataset...")
    stats = analyze_dataset_stats(records, image_mapping)
    print_statistics(stats)
    
    if args.stats_only:
        print("\nâœ… Statistics analysis complete (stats-only mode)")
        return
    
    # Convert to LLaVA format
    print(f"\nğŸ”„ Converting to LLaVA format...")
    llava_records = []
    
    pbar = tqdm(records, desc="Converting", unit="records", ncols=100)
    for record in pbar:
        llava_record = convert_record_to_llava(
            record,
            args.question_template,
            args.image_ext,
            args.min_prompt_length,
            args.max_prompt_length,
            image_mapping
        )
        if llava_record:
            llava_records.append(llava_record)
        
        # Update progress bar with current count
        pbar.set_postfix(converted=len(llava_records))
    
    pbar.close()
    
    if not llava_records:
        print("âŒ No valid records found for conversion")
        return
    
    # Save output
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    print(f"\nğŸ’¾ Saving {len(llava_records):,} LLaVA records to: {output_path}")
    
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(llava_records, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âŒ Error saving output file: {e}")
        return
    
    # Final statistics
    file_size_mb = output_path.stat().st_size / 1024 / 1024
    conversion_rate = (len(llava_records) / len(records)) * 100
    
    print("\n" + "="*70)
    print("ğŸ‰ CONVERSION COMPLETE")
    print("="*70)
    print(f"ğŸ“ Output file: {output_path}")
    print(f"ğŸ“Š Records converted: {len(llava_records):,} / {len(records):,}")
    print(f"ğŸ“ˆ Success rate: {conversion_rate:.2f}%")
    print(f"ğŸ’¾ File size: {file_size_mb:.2f} MB")
    print(f"ğŸ–¼ï¸  Image directory: {args.image_dir}")
    print(f"ğŸ“¸ Total images available: {len(image_mapping):,}")
    
    print(f"\nğŸ’¡ Usage Tips:")
    print(f"   - Use this file directly with LLaVA training frameworks")
    print(f"   - Images paths are relative to: {args.image_dir}")
    print(f"   - Only records with existing images are included")
    print(f"   - Question template: '{args.question_template}'")
    print(f"   - Prompt length range: {args.min_prompt_length}-{args.max_prompt_length} chars")


if __name__ == "__main__":
    main()