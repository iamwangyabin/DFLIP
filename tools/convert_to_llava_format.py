#!/usr/bin/env python3
"""Convert consolidated JSON metadata to LLaVA format for multimodal training.

This script processes the consolidated JSON file generated by consolidate_json_files.py
and converts records with valid prompts into LLaVA conversation format.

Usage:
    python tools/convert_to_llava_format.py \
        --input consolidated.json \
        --output llava_dataset.json \
        --image-ext .jpg
"""

import argparse
import json
import re
from pathlib import Path
from typing import List, Dict, Any, Optional
from tqdm import tqdm
from collections import defaultdict


def parse_args():
    parser = argparse.ArgumentParser(
        description="Convert consolidated JSON to LLaVA format"
    )
    parser.add_argument(
        "--input",
        type=str,
        required=True,
        help="Input consolidated JSON file path"
    )
    parser.add_argument(
        "--output", 
        type=str,
        required=True,
        help="Output LLaVA format JSON file path"
    )
    parser.add_argument(
        "--image-ext",
        type=str,
        default=".jpg",
        help="Image file extension (default: .jpg)"
    )
    parser.add_argument(
        "--question-template",
        type=str,
        default="<image>\nDescribe this image in detail.",
        help="Question template for human input"
    )
    parser.add_argument(
        "--min-prompt-length",
        type=int,
        default=10,
        help="Minimum prompt length to include (default: 10)"
    )
    parser.add_argument(
        "--max-prompt-length",
        type=int,
        default=2000,
        help="Maximum prompt length to include (default: 2000)"
    )
    parser.add_argument(
        "--max-records",
        type=int,
        default=None,
        help="Maximum number of records to process (for testing)"
    )
    parser.add_argument(
        "--stats-only",
        action="store_true",
        help="Only show statistics without generating output file"
    )
    return parser.parse_args()


def clean_prompt(prompt: str) -> str:
    """Clean and normalize prompt text."""
    if not prompt:
        return ""
    
    # Remove excessive whitespace
    prompt = re.sub(r'\s+', ' ', prompt.strip())
    
    # Remove common unwanted patterns
    unwanted_patterns = [
        r'\bfully clothed no boobs\b',
        r'\bno nudity\b',
        r'\bsfw\b',
        r'\bnsfw\b',
    ]
    
    for pattern in unwanted_patterns:
        prompt = re.sub(pattern, '', prompt, flags=re.IGNORECASE)
    
    # Clean up extra spaces after removal
    prompt = re.sub(r'\s+', ' ', prompt.strip())
    
    return prompt


def is_valid_prompt(prompt: str, min_length: int, max_length: int) -> bool:
    """Check if prompt is valid for training."""
    if not prompt or not isinstance(prompt, str):
        return False
    
    cleaned = clean_prompt(prompt)
    
    # Check length constraints
    if len(cleaned) < min_length or len(cleaned) > max_length:
        return False
    
    # Check for meaningful content (not just punctuation/numbers)
    if not re.search(r'[a-zA-Z]', cleaned):
        return False
    
    return True


def extract_prompt_from_record(record: Dict[str, Any]) -> Optional[str]:
    """Extract prompt from a record, handling different possible locations."""
    # Try meta.prompt first (most common)
    if record.get('meta') and isinstance(record['meta'], dict):
        prompt = record['meta'].get('prompt')
        if prompt:
            return str(prompt)
    
    # Try other possible locations
    prompt_fields = ['prompt', 'description', 'caption']
    for field in prompt_fields:
        if record.get(field):
            return str(record[field])
    
    return None


def convert_record_to_llava(record: Dict[str, Any], question_template: str, 
                           image_ext: str, min_length: int, max_length: int) -> Optional[Dict[str, Any]]:
    """Convert a single record to LLaVA format."""
    # Extract ID
    record_id = record.get('id')
    if not record_id:
        return None
    
    # Extract prompt
    raw_prompt = extract_prompt_from_record(record)
    if not raw_prompt:
        return None
    
    # Clean and validate prompt
    cleaned_prompt = clean_prompt(raw_prompt)
    if not is_valid_prompt(cleaned_prompt, min_length, max_length):
        return None
    
    # Generate image filename
    image_filename = f"{record_id}{image_ext}"
    
    # Create LLaVA format record
    llava_record = {
        "id": str(record_id),
        "image": image_filename,
        "conversations": [
            {
                "from": "human",
                "value": question_template
            },
            {
                "from": "gpt", 
                "value": cleaned_prompt
            }
        ]
    }
    
    return llava_record


def analyze_dataset_stats(records: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Analyze dataset statistics."""
    stats = {
        'total_records': len(records),
        'records_with_meta': 0,
        'records_with_prompt': 0,
        'valid_prompts': 0,
        'prompt_lengths': [],
        'model_families': defaultdict(int),
        'base_models': defaultdict(int)
    }
    
    for record in records:
        # Count records with meta
        if record.get('meta'):
            stats['records_with_meta'] += 1
        
        # Count records with prompts
        prompt = extract_prompt_from_record(record)
        if prompt:
            stats['records_with_prompt'] += 1
            cleaned = clean_prompt(prompt)
            if cleaned:
                stats['prompt_lengths'].append(len(cleaned))
                if is_valid_prompt(cleaned, 10, 2000):
                    stats['valid_prompts'] += 1
        
        # Count model families
        if record.get('_meta_family'):
            stats['model_families'][record['_meta_family']] += 1
        
        # Count base models
        if record.get('baseModel'):
            stats['base_models'][record['baseModel']] += 1
    
    # Calculate prompt length statistics
    if stats['prompt_lengths']:
        stats['avg_prompt_length'] = sum(stats['prompt_lengths']) / len(stats['prompt_lengths'])
        stats['min_prompt_length'] = min(stats['prompt_lengths'])
        stats['max_prompt_length'] = max(stats['prompt_lengths'])
    else:
        stats['avg_prompt_length'] = 0
        stats['min_prompt_length'] = 0
        stats['max_prompt_length'] = 0
    
    return stats


def print_statistics(stats: Dict[str, Any]):
    """Print dataset statistics in a formatted way."""
    print("\n" + "="*70)
    print("ğŸ“Š DATASET STATISTICS")
    print("="*70)
    
    print(f"ğŸ“ Total records: {stats['total_records']:,}")
    print(f"ğŸ”§ Records with metadata: {stats['records_with_meta']:,}")
    print(f"ğŸ’¬ Records with prompts: {stats['records_with_prompt']:,}")
    print(f"âœ… Valid prompts for training: {stats['valid_prompts']:,}")
    
    if stats['valid_prompts'] > 0:
        conversion_rate = (stats['valid_prompts'] / stats['total_records']) * 100
        print(f"ğŸ“ˆ Conversion rate: {conversion_rate:.2f}%")
    
    print(f"\nğŸ“ Prompt Length Statistics:")
    print(f"   Average: {stats['avg_prompt_length']:.1f} characters")
    print(f"   Range: {stats['min_prompt_length']} - {stats['max_prompt_length']} characters")
    
    print(f"\nğŸ·ï¸  Top Model Families:")
    for family, count in sorted(stats['model_families'].items(), 
                               key=lambda x: x[1], reverse=True)[:5]:
        print(f"   {family}: {count:,} records")
    
    print(f"\nğŸ¤– Top Base Models:")
    for model, count in sorted(stats['base_models'].items(), 
                              key=lambda x: x[1], reverse=True)[:5]:
        print(f"   {model}: {count:,} records")


def main():
    args = parse_args()
    
    input_path = Path(args.input)
    if not input_path.exists():
        print(f"âŒ Input file not found: {input_path}")
        return
    
    print(f"ğŸ“‚ Loading consolidated JSON from: {input_path}")
    
    # Load input data
    try:
        with open(input_path, 'r', encoding='utf-8') as f:
            records = json.load(f)
    except Exception as e:
        print(f"âŒ Error loading input file: {e}")
        return
    
    if not isinstance(records, list):
        print("âŒ Input file should contain a JSON array")
        return
    
    print(f"ğŸ“Š Loaded {len(records):,} records")
    
    # Limit records if specified
    if args.max_records:
        records = records[:args.max_records]
        print(f"ğŸ”¢ Limited to {len(records):,} records for processing")
    
    # Analyze statistics
    print("\nğŸ” Analyzing dataset...")
    stats = analyze_dataset_stats(records)
    print_statistics(stats)
    
    if args.stats_only:
        print("\nâœ… Statistics analysis complete (stats-only mode)")
        return
    
    # Convert to LLaVA format
    print(f"\nğŸ”„ Converting to LLaVA format...")
    llava_records = []
    
    pbar = tqdm(records, desc="Converting", unit="records", ncols=100)
    for record in pbar:
        llava_record = convert_record_to_llava(
            record, 
            args.question_template,
            args.image_ext,
            args.min_prompt_length,
            args.max_prompt_length
        )
        if llava_record:
            llava_records.append(llava_record)
        
        # Update progress bar with current count
        pbar.set_postfix(converted=len(llava_records))
    
    pbar.close()
    
    if not llava_records:
        print("âŒ No valid records found for conversion")
        return
    
    # Save output
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    print(f"\nğŸ’¾ Saving {len(llava_records):,} LLaVA records to: {output_path}")
    
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(llava_records, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âŒ Error saving output file: {e}")
        return
    
    # Final statistics
    file_size_mb = output_path.stat().st_size / 1024 / 1024
    conversion_rate = (len(llava_records) / len(records)) * 100
    
    print("\n" + "="*70)
    print("ğŸ‰ CONVERSION COMPLETE")
    print("="*70)
    print(f"ğŸ“ Output file: {output_path}")
    print(f"ğŸ“Š Records converted: {len(llava_records):,} / {len(records):,}")
    print(f"ğŸ“ˆ Success rate: {conversion_rate:.2f}%")
    print(f"ğŸ’¾ File size: {file_size_mb:.2f} MB")
    
    print(f"\nğŸ’¡ Usage Tips:")
    print(f"   - Use this file directly with LLaVA training frameworks")
    print(f"   - Images should be named as: <id>{args.image_ext}")
    print(f"   - Question template: '{args.question_template}'")
    print(f"   - Prompt length range: {args.min_prompt_length}-{args.max_prompt_length} chars")


if __name__ == "__main__":
    main()