"""
Formatting utilities for DFLIP dataset.
Converts Stage 1 outputs into natural language for Stage 2 training.
"""

from typing import Dict, List, Optional


def format_stage1_output(
    is_fake: bool,
    generator: Optional[str] = None,
    confidence: Optional[float] = None
) -> str:
    """
    Format Stage 1 detection/identification results into natural language.
    
    Args:
        is_fake: Whether the image is detected as fake
        generator: Name of the generator model (if fake)
        confidence: Detection confidence (optional)
    
    Returns:
        Natural language description of Stage 1 results
    """
    if not is_fake:
        text = "This image appears to be a **real photograph**."
        if confidence is not None:
            text += f" (Confidence: {confidence:.2%})"
        return text
    
    text = "This image appears to be **AI-generated**."
    if generator:
        text += f" Generated by: **{generator}**."
    if confidence is not None:
        text += f" (Confidence: {confidence:.2%})"
    
    return text


def format_stage2_conversation(
    is_fake: bool,
    generator: Optional[str] = None,
    gt_prompt: Optional[str] = None,
    include_assistant: bool = True,
    stage1_output: Optional[str] = None
) -> List[Dict[str, str]]:
    """
    Format a conversation for Stage 2 (Interpreter) training/inference.
    
    This creates a conversation in the format expected by Qwen VL:
    [
        {"role": "system", "content": "..."},
        {"role": "user", "content": "..."},
        {"role": "assistant", "content": "..."}  # Only if include_assistant=True
    ]
    
    Args:
        is_fake: Whether image is fake (from Stage 1 or ground truth)
        generator: Generator name (from Stage 1 or ground truth)
        gt_prompt: Ground truth prompt (only for training)
        include_assistant: Whether to include assistant response (True for training)
        stage1_output: Pre-formatted Stage 1 output (overrides is_fake/generator)
    
    Returns:
        List of message dictionaries for conversation format
    """
    
    # System message
    system_message = {
        "role": "system",
        "content": (
            "You are DFLIP, an expert AI system for analyzing images and detecting deepfakes. "
            "Your task is to analyze the given image and provide detailed insights about:\n"
            "1. Whether it is real or AI-generated\n"
            "2. If AI-generated, which model likely created it\n"
            "3. If AI-generated, predict the text prompt used to generate it\n\n"
            "Provide your analysis in a clear, structured format."
        )
    }
    
    # User message - includes image token placeholder
    if stage1_output:
        profiling_context = stage1_output
    else:
        profiling_context = format_stage1_output(is_fake, generator)
    
    user_content = (
        "Please analyze this image for potential deepfake indicators.\n\n"
        f"**Initial Profiling Results:**\n{profiling_context}\n\n"
        "Based on this analysis, please provide:\n"
        "1. Your assessment of whether this is real or AI-generated\n"
        "2. If AI-generated, the most likely generator model\n"
        "3. If AI-generated, predict the text prompt that was used to create this image"
    )
    
    user_message = {
        "role": "user",
        "content": user_content
    }
    
    messages = [system_message, user_message]
    
    # Assistant message (for training only)
    if include_assistant:
        if not is_fake:
            assistant_content = (
                "**Analysis Result:**\n\n"
                "**1. Authenticity:** This is a **real photograph**, not AI-generated.\n\n"
                "**2. Generator Model:** N/A (Real image)\n\n"
                "**3. Generation Prompt:** N/A (Real image)\n\n"
                "This image shows no indicators of AI generation and appears to be captured "
                "by a camera or created through traditional photography methods."
            )
        else:
            prompt_section = f"**Predicted Prompt:**\n```\n{gt_prompt}\n```" if gt_prompt else "N/A"
            
            assistant_content = (
                "**Analysis Result:**\n\n"
                "**1. Authenticity:** This image is **AI-generated** (deepfake).\n\n"
                f"**2. Generator Model:** {generator or 'Unknown'}\n\n"
                f"**3. Generation Prompt:**\n{prompt_section}\n\n"
                "The image exhibits typical characteristics of AI-generated content from this model."
            )
        
        assistant_message = {
            "role": "assistant",
            "content": assistant_content
        }
        messages.append(assistant_message)
    
    return messages


def format_inference_prompt(
    detection_result: Dict,
    include_localization: bool = True
) -> str:
    """
    Format Stage 1 detection results for Stage 2 inference input.
    
    Args:
        detection_result: Dictionary containing Stage 1 outputs
            {
                'is_fake': bool or float (probability),
                'generator_id': int,
                'generator_name': str,
                'confidence': float,
                'has_forgery_mask': bool
            }
        include_localization: Whether to mention forgery localization
    
    Returns:
        Formatted prompt string for Stage 2
    """
    is_fake = detection_result.get('is_fake', False)
    if isinstance(is_fake, float):
        is_fake = is_fake > 0.5
    
    confidence = detection_result.get('confidence', None)
    generator = detection_result.get('generator_name', 'Unknown')
    has_mask = detection_result.get('has_forgery_mask', False)
    
    base_output = format_stage1_output(is_fake, generator, confidence)
    
    if include_localization and has_mask and is_fake:
        base_output += "\n**Forgery localization:** Manipulated regions have been detected."
    
    return base_output


def parse_assistant_response(response: str) -> Dict[str, str]:
    """
    Parse the assistant's response to extract structured information.
    
    Args:
        response: Raw text response from the LLM
    
    Returns:
        Dictionary with parsed fields:
            {
                'is_fake': str,
                'generator': str,
                'prompt': str
            }
    """
    result = {
        'is_fake': '',
        'generator': '',
        'prompt': ''
    }
    
    lines = response.split('\n')
    
    # Simple parsing - can be made more robust
    for i, line in enumerate(lines):
        if 'Authenticity:' in line or '1.' in line:
            result['is_fake'] = line.split('**')[-2] if '**' in line else line
        elif 'Generator Model:' in line or '2.' in line:
            result['generator'] = line.split('**')[-2] if '**' in line else line
        elif 'Predicted Prompt:' in line or 'Generation Prompt:' in line or '3.' in line:
            # Try to extract prompt from code block
            if '```' in response[i:]:
                code_block_start = response.find('```', response.find(line))
                code_block_end = response.find('```', code_block_start + 3)
                if code_block_end > code_block_start:
                    result['prompt'] = response[code_block_start+3:code_block_end].strip()
    
    return result
